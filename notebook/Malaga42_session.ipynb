{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8b53ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26e9840",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(os.path.join('..', '.env'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de2c2dc-2c36-4f08-a4a5-08a2c6aa8376",
   "metadata": {},
   "source": [
    "# 1. OpenAI API and its Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d718d084-0d79-414d-af5e-773a67071b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac576ff-0917-4eed-a2ed-339dec82e1e0",
   "metadata": {},
   "source": [
    "Let's look at how the OpenAI API works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33237a0-8fbd-4811-bedf-6544f2d4d246",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the closest star to Earth?\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": question\n",
    "    }\n",
    "]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=messages,\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbee423-68f5-4577-a552-7e6b04efd7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb44a80-3498-4579-b900-fbf3a81e4d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.choices[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926d2fe3-6dd2-4193-8d68-b4abec3675ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fbf448-5644-4852-9308-1e5a3c0e8e6c",
   "metadata": {},
   "source": [
    "This is the answer \"message\". Note that the AI is called \"assistant\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d22ce9-2438-48fd-a8f0-39d692ac811e",
   "metadata": {},
   "source": [
    "Another important information is the number of tokens used, as that tells us the cost of the API call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d985aea8-96a8-4e97-96c4-2945a995af43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75150791-a4b5-47dd-b796-46848822b045",
   "metadata": {},
   "source": [
    "Looking up the costs on openai website, we find:\n",
    "- 0.0015 USD per 1000 prompt tokens\n",
    "- 0.0020 USD per 1000 completion tokens\n",
    "\n",
    "So, the total cost is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e496325-af7b-44a9-b814-335b63e6d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_cost = 0.0015\n",
    "completion_cost = 0.0020\n",
    "\n",
    "cost = (response.usage.prompt_tokens * prompt_cost + response.usage.completion_tokens * completion_cost)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16a86e0-9a50-439f-aaea-aa1ec8c8565b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{cost:.6f} USD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad103fe9-3d49-4a30-a311-0a26213872a1",
   "metadata": {},
   "source": [
    "Let's wrap this together in a class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294728d4-4bea-4b89-b069-98f94d85b87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAgent:\n",
    "    \n",
    "    def __init__(self, model=\"gpt-3.5-turbo\", prices=[0.0015, 0.0020]):\n",
    "        self.model = model\n",
    "        self.prompt_cost = prices[0]\n",
    "        self.completion_cost = prices[1]\n",
    "        self.cost_list = []\n",
    "        self.total_cost = 0\n",
    "        \n",
    "    def reply(self, question):\n",
    "        \n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        self.calculate_cost(response.usage)\n",
    "        \n",
    "        message = response.choices[0].message\n",
    "        \n",
    "        print(message.content)\n",
    "\n",
    "    def calculate_cost(self, usage):\n",
    "        cost = (usage.prompt_tokens * self.prompt_cost + usage.completion_tokens * self.completion_cost)/1000\n",
    "        self.cost_list.append(cost)\n",
    "        self.total_cost += cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b166643-1916-4c84-bf00-527cbcb13b1d",
   "metadata": {},
   "source": [
    "Test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a000b354-4473-463a-8eb1-d7395f243800",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = BaseAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e483d1-29b5-44b4-b12f-cc4a0b2f2f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.reply(\"What is the closest star to Earth?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7f4e4c-eb4a-4531-b035-d0d7c4d040c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{agent.total_cost:.6f} USD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56202847-c8b0-4d6d-aaec-ad12d79e9e32",
   "metadata": {},
   "source": [
    "Let's ask it another question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9e36a8-9d4f-4d37-bdce-c0eb7f8bf790",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.reply(\"What does 'USD' stand for in the forex context?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32561b87-08d9-4bc0-a900-225aa7d71fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{agent.total_cost:.6f} USD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cc8fb6-4a5a-424c-8c70-5062a7d2e5d1",
   "metadata": {},
   "source": [
    "We can see that `total_cost` is accumulating the costs of all questions asked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8b1233-b243-46ab-86de-4e9b7fed8e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(agent.cost_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb17a0e-bf0e-43f0-b925-90d9d7ff7cce",
   "metadata": {},
   "source": [
    "# 2. It's Hard to have a Conversation with Someone that doesn't Listen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d7574a-1a8f-424c-89a4-514a5d691def",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = BaseAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451a3a1c-fb19-4b19-893d-8fa49e684938",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.reply(\"Hi, my name is Andrea.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777ac88c-f850-46ed-9385-ab25366eb282",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.reply(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36142497-2ea9-449c-900d-75e2ec068005",
   "metadata": {},
   "source": [
    "Problem: the each call to the LLM is independent than the previous one, so the AI is ignoring the history of the conversation. To fix this, we need to give it a \"memory\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5508813a-5126-4b5a-abe1-37f3e103f54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.messages = []\n",
    "        \n",
    "    def add(self, message):\n",
    "        self.messages.append(dict(message))\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"\\n\".join([str(message) for message in self.messages])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78153ff-1d26-400b-9401-8f262fdcde58",
   "metadata": {},
   "source": [
    "Let's modify the `BaseAgent` class to make use of the memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faafe02-b01b-4f5f-8448-a74ddd0d29d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAgent:\n",
    "    \n",
    "    def __init__(self, memory, model=\"gpt-3.5-turbo\", prices=[0.0015, 0.0020]):\n",
    "        self.memory = memory\n",
    "        self.model = model\n",
    "        self.prompt_cost = prices[0]\n",
    "        self.completion_cost = prices[1]\n",
    "        self.cost_list = []\n",
    "        self.total_cost = 0\n",
    "        \n",
    "    def reply(self, question):\n",
    "        \n",
    "        human_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question\n",
    "        }\n",
    "        self.memory.add(human_message)\n",
    "        \n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=self.model,\n",
    "            messages=self.memory.messages,\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        self.calculate_cost(response.usage)\n",
    "        \n",
    "        agent_message = response.choices[0].message\n",
    "        self.memory.add(agent_message)\n",
    "        \n",
    "        print(agent_message.content)\n",
    "\n",
    "    def calculate_cost(self, usage):\n",
    "        cost = (usage.prompt_tokens * self.prompt_cost + usage.completion_tokens * self.completion_cost)/1000\n",
    "        self.cost_list.append(cost)\n",
    "        self.total_cost += cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647f24fa-1f2e-4e45-8205-f9adc0771d33",
   "metadata": {},
   "source": [
    "Test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66776315-8559-4035-8061-57c0a9b80be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory()\n",
    "agent = BaseAgent(memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b8c050-bc22-4f6f-8cb0-3c8b65b7c41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.reply(\"Hi, my name is Andrea.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e4e53d-484d-4958-a425-07fbd60a7974",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab70f95f-492c-4754-9478-9f82bf349180",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.reply(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69f1684-3f5e-42d4-a790-d8536a93d45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b16913-bbf1-4f6c-bea4-765ccd4adfc6",
   "metadata": {},
   "source": [
    "With each exchange in the conversation, the whole history has to be sent as input to the LLM. Problems:\n",
    "\n",
    "1. LLMs have a finite input size\n",
    "2. providers (such as OpenAI) charge based on the number of tokens\n",
    "\n",
    "To solve these issues, as the memory grows, we must start deleting the oldest history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e99b4f-a2d7-46cf-af21-8bb07e778bdf",
   "metadata": {},
   "source": [
    "To solve the problem, we define a `delete_history` method in the `Memory` class, which counts the number of tokens of the conversation and, if that is larger than `max_tokens`, it deletes the first message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dff0b7-e2ee-49c9-b0d4-9e9a2cce42a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f94863-c564-405b-a36d-997950dc7e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    \n",
    "    def __init__(self, max_tokens=3000):\n",
    "        self.messages = []\n",
    "        self.max_tokens = max_tokens\n",
    "        self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        \n",
    "    def add(self, message):\n",
    "        self.messages.append(dict(message))\n",
    "\n",
    "    def delete_history(self):\n",
    "        while True:\n",
    "            total_tokens = 0\n",
    "            for message in self.messages:\n",
    "                message_tokens = len(self.encoding.encode(message[\"content\"]))\n",
    "                total_tokens += message_tokens\n",
    "            if total_tokens > self.max_tokens:\n",
    "                self.messages = self.messages[1:]\n",
    "            else:\n",
    "                return\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"\\n\".join([str(message) for message in self.messages])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c835a3-3bfd-4c63-8533-1d72e3f9d48f",
   "metadata": {},
   "source": [
    "We then modify `BaseAgent` so as to delete the history every time the API is called. We do so by moving the logic to a new method `generate_response`, so that `reply` is cleaner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9613f90-ad08-4410-9a4a-881cbfee4e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAgent:\n",
    "    \n",
    "    def __init__(self, memory, model=\"gpt-3.5-turbo\", prices=[0.0015, 0.0020]):\n",
    "        self.memory = memory\n",
    "        self.model = model\n",
    "        self.prompt_cost = prices[0]\n",
    "        self.completion_cost = prices[1]\n",
    "        self.cost_list = []\n",
    "        self.total_cost = 0\n",
    "        \n",
    "    def reply(self, question):\n",
    "\n",
    "        human_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question\n",
    "        }\n",
    "        self.memory.add(human_message)\n",
    "        \n",
    "        agent_message = self.generate_response()\n",
    "        self.memory.add(agent_message)\n",
    "\n",
    "        print(agent_message.content)\n",
    "\n",
    "    def generate_response(self):\n",
    "        \n",
    "        self.memory.delete_history()\n",
    "        \n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=self.model,\n",
    "            messages=self.memory.messages,\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        self.calculate_cost(response.usage)\n",
    "\n",
    "        agent_message = response.choices[0].message\n",
    "        \n",
    "        return agent_message\n",
    "    \n",
    "    def calculate_cost(self, usage):\n",
    "        cost = (usage.prompt_tokens * self.prompt_cost + usage.completion_tokens * self.completion_cost)/1000\n",
    "        self.cost_list.append(cost)\n",
    "        self.total_cost += cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c01f0c-3068-486c-985a-c0fa5c81c043",
   "metadata": {},
   "source": [
    "Test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555bb480-0d5f-4879-ad74-2d6665452750",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(max_tokens=30)\n",
    "agent = BaseAgent(memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e50a92b-067f-4b36-a009-4d6359c5527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.reply(\"My name is Andrea.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f28cce5-78a6-4404-adfd-152949c23550",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceaeb40-ea6c-4b2f-972c-ab5e5e360b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.reply(\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc96412-99c4-433d-b7a4-f2e51d2ec67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f35e701-f48a-4745-8ca3-5ab8786b7a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.reply(\"What is the most common word in English?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e3175d-1abd-4ace-8eaf-8bb983cebefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f4423a-c058-4dd7-b33d-044963c66453",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.reply(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6230e2c7-9105-47b7-ae79-4d96e03ea710",
   "metadata": {},
   "source": [
    "By playing with `max_tokens`, we can control the trade-off between larger context (and hence more reliable answer) and lower costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb6829e-3330-4f76-9897-f1ee496a9315",
   "metadata": {},
   "source": [
    "# 3. Initial Prompt and Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5431cba5-ac47-4a6c-958d-fecd111e7653",
   "metadata": {},
   "source": [
    "Until now we have not specified any \"initial instruction\" to the AI, so the only input it takes is the user's question. But what if we want the agent to act in a specific way, e.g., to talk only in one language, or to only focus on a specific subject?\n",
    "\n",
    "This can be done by passing to it an initial prompt, which in OpenAI is nothing but a message with the \"system\" role. Let's therefore add this to the `Memory` class, and modify the `delete_history` so as never to delete this first message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560566ad-3dcd-425b-8e97-a5c0978bbc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    \n",
    "    def __init__(self, initial_prompt=None, max_tokens=3000):\n",
    "        if initial_prompt is None:\n",
    "            initial_prompt = \"\"\n",
    "        self.messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": initial_prompt\n",
    "            }\n",
    "        ]\n",
    "        self.max_tokens = max_tokens\n",
    "        self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        \n",
    "    def add(self, message):\n",
    "        self.messages.append(dict(message))\n",
    "\n",
    "    def delete_history(self):\n",
    "        while True:\n",
    "            total_tokens = 0\n",
    "            for message in self.messages:\n",
    "                message_tokens = len(self.encoding.encode(message[\"content\"]))\n",
    "                total_tokens += message_tokens\n",
    "            if total_tokens > self.max_tokens:\n",
    "                self.messages[1:] = self.messages[2:]\n",
    "            else:\n",
    "                return\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"\\n\".join([str(message) for message in self.messages])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defda509-f7c3-4af5-9a94-8f30e03d979a",
   "metadata": {},
   "source": [
    "Test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60c1edb-839d-4b24-a4a2-57a16dcb9897",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_prompt = \"\"\"\n",
    "You are a finance expert. Answer the user's financial question providing technical details if needed.\n",
    "If a question is not about finance, politely decline to answer, as that is beyond your scope and expertise.\n",
    "\"\"\"\n",
    "\n",
    "memory = Memory(initial_prompt=initial_prompt)\n",
    "agent = BaseAgent(memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84488e48-c505-4c18-9799-5ad76db81ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.reply(\"What are some of the main drivers of FX volatility?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda98397-459e-4777-8e4b-2b23311fd065",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.reply(\"How many stars are there in the Milky Way?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cd4112-b165-4440-999a-01c33b03a1d8",
   "metadata": {},
   "source": [
    "This is just modifying the style of the AI, but you can use the prompt to fundamentally alter the nature of the agent. For example, you may use it as an \"intention classifier\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38711b28-a406-4f42-b336-d0d3531083c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_prompt = \"\"\"\n",
    "The user will provide a restaurant review.\n",
    "If it's positive, output 1; if it's negative, output -1; in all other cases, output 0.\n",
    "\"\"\"\n",
    "\n",
    "memory = Memory(initial_prompt=initial_prompt)\n",
    "agent = BaseAgent(memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e05c76-632e-482b-ab92-0f173e98dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_review = \"\"\"\n",
    "Our visit was perfect! The place is outstanding and comfy. People are gentle and well educated.\n",
    "Food is amazing!! They serve a 5-course menu and everything is delicious! Their wine menu is one of the best!!!\n",
    "We had a very good time and took home sweet memories! For sure on our next visit to the city we’ll go back there!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a20d250-726d-4216-96b1-83e84cabf95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.reply(user_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e31461-2d74-49e6-a220-5bb89b91473c",
   "metadata": {},
   "source": [
    "In some cases, a little prompt engineering can make the model from bad to good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af14de1-80f8-4e2d-b5c2-9216896cc000",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_prompt = \"\"\"\n",
    "The user will provide a restaurant review. From it, determine if the restaurant should be recommended to a vegan person.\n",
    "Provide a reasoning for your answer. Output your reasoning. Then in the new line output 1 if yes, -1 if no, and 0 in all other cases.\n",
    "\"\"\"\n",
    "\n",
    "memory = Memory(initial_prompt=initial_prompt)\n",
    "agent = BaseAgent(memory=memory)\n",
    "\n",
    "agent.reply(user_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130ea33f-e8e1-4cf6-bc66-1bc3ba543b54",
   "metadata": {},
   "source": [
    "The final use case we look in, is if we want to have a conversation based on a specific context. For example, you could pass in the prompt a user-provided text, which the user can then query via the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036c9da5-d5c4-4915-98e1-912087f5a7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "webpage_content = \"\"\"\n",
    "Ebury Logo\n",
    "About us\n",
    "Corporates\n",
    "Institutions\n",
    "Partner with us\n",
    "Careers\n",
    "Login\n",
    "Powering growth beyond borders.\n",
    "From payments, collections, risk management, financing and more – we help businesses maximise their global growth potential.\n",
    "\n",
    "OUR STORY\n",
    "\n",
    "We make international trade more accessible, simple and personal.\n",
    "We believe in a world where any business, big or small, should be able to transact globally with the same ease\n",
    "and reliability they experience locally. We bring together our in-house platform,\n",
    "in-depth expertise and custom solutions to help businesses go borderless and achieve their ambitions faster.\n",
    "\n",
    "\n",
    "£ 27 B transacted in FY2022\n",
    "\n",
    "1 M+ payments processed in the last 12 months\n",
    "\n",
    "50,000 + clients served worldwide\n",
    "\n",
    "1,600 + employees in 21 countries\n",
    "\n",
    "Meet our team Leadership:\n",
    "\n",
    "Peter Holmes, SVP of Client Onboarding\n",
    "Richard Hughes, SVP of Credit Risk\n",
    "Toby Young, Group Technology Director\n",
    "Venancio Gallego, Strategic Advisor\n",
    "Zafeer Ahmed, Global Head of Dealing\n",
    "Ana Muñoz Fenollosa, Group Financial Director\n",
    "Duane Swailes, SVP of Sales Acceleration & Marketing\n",
    "Enrique Colin, SVP of Product and Data\n",
    "Enrique Diaz-Alvarez, Chief Risk Officer\n",
    "Fernando Pierri, Chief Commercial Officer\n",
    "Juan Lobato, Founder & CEO\n",
    "\n",
    "JOIN OUR TEAM\n",
    "Join us as we build the international trade platform of the future and transform how businesses transact globally.\n",
    "\n",
    "Company\n",
    "Our story\n",
    "Press room\n",
    "Our global presence\n",
    "Careers\n",
    "Resource Hub\n",
    "Blog\n",
    "Podcast\n",
    "Ebury Labs\n",
    "Help Centre\n",
    "Corporate solutions\n",
    "E-commerce\n",
    "NGO's and charities\n",
    "Mass Payments\n",
    "Corporate products\n",
    "Payments and collections\n",
    "Digital platforms\n",
    "Business lending\n",
    "FX risk management\n",
    "Institutions\n",
    "Ebury Institutional Solutions\n",
    "Partner with us\n",
    "White Label Solution\n",
    "Branded affiliates\n",
    "Affiliates\n",
    "\n",
    "Get in touch with us\n",
    "We’re happy to help! Contact us to learn more.\n",
    "\n",
    "Subscribe to our blog   \n",
    "Expert insights to grow your business globally.\n",
    "\n",
    "FX Talk an Ebury podcast   \n",
    "Get a breakdown of the global markets from our experts.\n",
    "\n",
    "Join our team   \n",
    "Explore open roles across 32+ offices worldwide.\n",
    "\n",
    "Legal Privacy Notice Cookie Notice Manage cookies\n",
    "United Kingdom - English\n",
    "\n",
    "Ebury Partners UK Ltd is authorised and regulated by the Financial Conduct Authority as an Electronic Money Institution. Reference number: 900797. Ebury Partners UK Ltd is registered with the Information Commissioners Office, with registration number: ZA345828. Ebury Partners Markets Ltd is authorised and regulated by the Financial Conduct Authority as an Investment Firm to provide advice and execute trades in MiFID derivative products. Reference number: 784063. EBURY and EBURY What Borders? are trademarks.\n",
    "\n",
    "Ebury Partners UK Ltd © 2023\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c5d723-bf74-4d28-b27d-94c4a2da05b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_prompt = f\"\"\"\n",
    "Answer the user's questions based solely on the following context, which comes from Ebury's website.\n",
    "\n",
    "CONTEXT:\n",
    "'''\n",
    "{webpage_content}\n",
    "'''\n",
    "\"\"\"\n",
    "\n",
    "memory = Memory(initial_prompt=initial_prompt)\n",
    "agent = BaseAgent(memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa7330c-7cf9-4190-a738-379d346ae3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.reply(\"What is the website about?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df940fbb-7b3d-4922-9f61-ed8a56462f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.reply(\"Who is Ebury's responsible for Client Onboarding?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2854c4f-280a-45cb-8c10-691b0d9d8d97",
   "metadata": {},
   "source": [
    "# 4. Embedding Vectors and Vector Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce1fd6f-a9a1-4d36-b596-425a49a47b7b",
   "metadata": {},
   "source": [
    "Consider a very domain-specific question, such as \"Can I export luxury goods to Russia in 2023?\". Likely, the LLM does not know that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4760a5-5d11-4d2c-94e4-37eef484d6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory()\n",
    "agent = BaseAgent(memory=memory)\n",
    "\n",
    "agent.reply(\"Can I export luxury goods to Russia in 2023?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b199c953-3f9d-4afb-9c3b-4a52c59c9f0f",
   "metadata": {},
   "source": [
    "How can we use the ideas above to improve the answers that the AI gives?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95fbcad-acd5-4ca7-84c2-95f46e9b622a",
   "metadata": {},
   "source": [
    "Suppose we have access to Ebury's internal documentation: the answer is in there, but we cannot paste the whole corpus into the context! We then need to identify the document which contains the answer, and in it, the paragraph that contains the answer. We are then going to provide only that as context.\n",
    "\n",
    "The way to \"find the relevant paragraph in the corpus\" is to use **embedding vectors**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd26f2f6-ef25-483d-83b2-08922254ce94",
   "metadata": {},
   "source": [
    "Let's start with the simplest type of embedding vector: a \"word embedding\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d36b929-bf72-4a70-83e9-7139ebc4c742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c329bc7-79e6-4d4c-9406-01a71ed54362",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7866e9bf-14a9-4972-ac63-86f74014608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = model.get_vector(\"beer\")\n",
    "\n",
    "len(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772c628a-629c-4caa-8828-c32eb3f98c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5e106a-e86c-4a76-9f0e-eecba008845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebf64e4-0957-4880-8e29-f4170a60529c",
   "metadata": {},
   "source": [
    "Let's try to compose vectors, and see if we find something interesting in their meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6256ad-9d29-4b65-bdcf-909e2aa0ec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec1 = model.get_vector(\"beer\")\n",
    "vec2 = model.get_vector(\"germany\")\n",
    "vec3 = model.get_vector(\"italy\")\n",
    "\n",
    "new_vec = vec1 - vec2 + vec3\n",
    "\n",
    "model.most_similar(new_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d3e0ca-93f0-41e8-8ca1-978e2a503918",
   "metadata": {},
   "source": [
    "Word embeddings generalize to document embedding: a sentence (and even a full text) can be transformed into a vector, which captures its semantic content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c675aca-f54e-4c9c-9f4b-fc4d6f062af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2b263a-a85c-4cc6-a7dc-35dcf6385c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "vec = embeddings.embed_query(\"The dog plays with the ball.\")\n",
    "\n",
    "len(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbc3ba8-a5b3-41cd-8111-de0907f13565",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec2 = embeddings.embed_query(\"The hound plays with the ball.\")\n",
    "vec3 = embeddings.embed_query(\"The cat plays with the ball.\")\n",
    "vec4 = embeddings.embed_query(\"Spain is hereby established as a social and democratic State.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3359894-1f88-4ca3-ae01-a925284a7929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f245637-074e-464e-b0e4-9453a89ccaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d542074-0276-4567-93d4-deef0a99379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(vec, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62a2fc1-2c12-41fd-b9c6-c2b87217e958",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(vec, vec3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabba356-ec62-4dbc-ba4e-f48851be0c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(vec, vec4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056d7336-2d66-4720-afb6-ee97691dc518",
   "metadata": {},
   "source": [
    "Now, the idea is simple: given all the documents in the corpus, we split them into text chunks (the \"value\"), and compute the embedding vector of each chunk (the \"key\"). We save each key-value pair in a database which, given its structure, is called a **vector database**.\n",
    "\n",
    "Then, when the user asks a question, we turn the question into an embedding vector, and look for the the most similar vectors among the database keys: we then retrieve the corresponding values (the text chunks)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69041c97-6d21-4bf8-946f-672aaf452844",
   "metadata": {},
   "source": [
    "Let's start constructing the vector database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39606717-6b77-4868-90b0-e92934c6a8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.document_loaders import UnstructuredPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250d8be7-5bdb-4350-a1bf-ba1404e052ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(chunk_size=300, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acd9033-4ada-4c9e-a157-42d61f9cfb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    os.path.join('..', 'data', 'sanctions-russia.pdf'),\n",
    "    os.path.join('..', 'data', 'X_FAQ.pdf'),\n",
    "    os.path.join('..', 'data', 'FX_FAQ.pdf')\n",
    "]\n",
    "\n",
    "all_docs = []\n",
    "for file in files:\n",
    "    loader = UnstructuredPDFLoader(file)\n",
    "    docs = loader.load_and_split(text_splitter=text_splitter)\n",
    "    all_docs = all_docs + docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90873776-a01c-4120-85a7-ffddc02598ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_docs[5].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0136ee73-7d81-483f-8930-6eeb98a5edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f895eb-95d5-4813-99d8-4d94c895c78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "vdb = FAISS.from_documents(all_docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e45890f-6429-4c8d-8554-fa3bb196bc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdb.save_local(os.path.join('..', 'src', 'vector_databases', 'my_database'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fac9218-f236-474b-9e85-9bacb3fdca55",
   "metadata": {},
   "source": [
    "Test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f467ef-be1c-4fcd-8aac-8fed3706f292",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = vdb.similarity_search(\"Can I export luxury goods to Russia in 2023?\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e053c-e607-48fa-aeb2-0dbd915b244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc_list[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee7b121-70ae-4309-bfe8-a68c09480edd",
   "metadata": {},
   "source": [
    "Assuming that this piece of text contains the answer to the question, we would now like to pass this to the chatbot as context. Of course, we can to this by hand (copy-paste), but then the bot is hardly automatic.\n",
    "\n",
    "Rather, we must give the LLM the **option** to search the vector database. In OpenAI, we can do so via the feature of **function call**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc962050-6989-4e76-80e9-7a8fe41d3e55",
   "metadata": {},
   "source": [
    "# 5. Funcion Call for RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c09ca88-5a3e-4987-adfd-480477ad17bf",
   "metadata": {},
   "source": [
    "How do we integrate the vector database search seen above with our chatbot?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b01461a-45b6-456d-8b07-7f2365ef1040",
   "metadata": {},
   "source": [
    "In OpenAI API, we do so using the following structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d50fd8-cc0f-4dc6-9a5e-349e4abef99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_documentation_string = \"\"\"\n",
    "{\n",
    "    \"name\": \"search_documentation\",\n",
    "    \"description\": \"Access information from internal documentation.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The user's query.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"query\"]\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fda43f-0137-47f7-86fc-a737cadcd011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "search_documentation_json = json.loads(search_documentation_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf813de-e3a1-4ab6-9486-a172397901a6",
   "metadata": {},
   "source": [
    "We now pass this string to the `openai.ChatCompletion.create` call (together with the messages):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e911462-7c4e-43b3-add8-5c26db7b7f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Can I export luxury goods to Russia in 2023?\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": question\n",
    "    }\n",
    "]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=messages,\n",
    "    functions=[search_documentation_json],\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da4eab5-3ab1-4893-868f-0c40f80386bb",
   "metadata": {},
   "source": [
    "Let's look at the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e1b2ba-c67a-4f16-a7f4-f80fe7fe7902",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = response[\"choices\"][0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a474c8-6091-436e-8633-434eb264fa32",
   "metadata": {},
   "source": [
    "The LLM did not output a message content, but rather a \"function call\". Inside it, it tells us the name of the function it wants to call, and the arguments.\n",
    "\n",
    "As you see, the LLM cannot **directly** run a function: it can only output text! But we can parse this output to actually run the function, and provide it the answer as a 'role' = 'function' message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3ebb92-4b6c-4aac-824c-30c03e2a1d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = json.loads(answer.message.function_call.arguments)['query']\n",
    "doc_list = vdb.similarity_search(\"Can I export luxury goods to Russia in 2023?\", k=5)\n",
    "function_output = \"\\n\\n\".join([doc.page_content for doc in doc_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cadb4cc-30e8-4f10-8906-fd85155ab228",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": question\n",
    "    },\n",
    "    answer.message,\n",
    "    {\n",
    "        \"role\": \"function\",\n",
    "        \"name\": \"search_documentation\",\n",
    "        \"content\": function_output\n",
    "    }\n",
    "]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=messages,\n",
    "    functions=[search_documentation_json],\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ede976-9552-4889-af7a-cc0a39284256",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = response[\"choices\"][0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84991c7-74e1-41b9-81c6-2b76a00eaeda",
   "metadata": {},
   "source": [
    "Now, we automate this process, giving the bot the possibility of querying the interal documenation, which we call the **knowledge base**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e74a2ad-697b-49e7-b987-459bea0d5d29",
   "metadata": {},
   "source": [
    "First, we define a new class `KnowledgeBase`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb1263e-8a33-4c7f-8b77-d55a5d50d2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fbd7f1-7cd1-4e30-abe3-f03c3f166395",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeBase:\n",
    "    \n",
    "    def __init__(self, vdb, max_chunks=5):\n",
    "        self.vdb = vdb\n",
    "        self.max_chunks = max_chunks\n",
    "        self.function_name = \"search_documentation\"\n",
    "        self.function = json.loads(self.search.__doc__)\n",
    "        \n",
    "    def search(self, query):\n",
    "        \"\"\"\n",
    "        {\n",
    "            \"name\": \"search_documentation\",\n",
    "            \"description\": \"Access information from internal documentation.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The user's query.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            }\n",
    "        }\n",
    "        \"\"\"\n",
    "        retrieved = self.vdb.similarity_search(query, k=self.max_chunks)\n",
    "        context = {}\n",
    "        for i, doc in enumerate(retrieved):\n",
    "            file_path = doc.metadata[\"source\"]\n",
    "            file_name = os.path.normpath(file_path).split(os.sep)[-1]\n",
    "            title = f\"INFORMATION {i + 1} (from {file_name})\"\n",
    "            content = re.sub(\"\\s+\", \" \", doc.page_content)\n",
    "            context[title] = content\n",
    "        return str(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7e4e2f-feca-40a7-b83e-cd60a1708507",
   "metadata": {},
   "source": [
    "The docstring of this function, jsonized into the attribute `function`, is what we pass to OpenAI's API: it tells the LLM that, if it wants to access information from internal documentation, it can do so by calling the function \"search_documentation\" with input the query (a string)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4f4f5b-bc99-4067-aea5-e95c66c552c9",
   "metadata": {},
   "source": [
    "In order for this to work, we must modify the `BaseAgent`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907f57be-2619-4fb7-a861-5ebdde9ee2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAgent:\n",
    "    \n",
    "    def __init__(self, memory, knowledge_base, model=\"gpt-3.5-turbo\", prices=[0.0015, 0.0020]):\n",
    "        self.memory = memory\n",
    "        self.knowledge_base = knowledge_base\n",
    "        self.model = model\n",
    "        self.prompt_cost = prices[0]\n",
    "        self.completion_cost = prices[1]\n",
    "        self.cost_list = []\n",
    "        self.total_cost = 0\n",
    "        \n",
    "    def reply(self, question):\n",
    "\n",
    "        human_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question\n",
    "        }\n",
    "        self.memory.add(human_message)\n",
    "\n",
    "        answer = False\n",
    "        while not answer:\n",
    "            \n",
    "            agent_message = self.generate_response()\n",
    "            self.memory.add(agent_message)\n",
    "            \n",
    "            if agent_message.content is not None:\n",
    "                answer = agent_message.content\n",
    "            \n",
    "            else:\n",
    "                function_call = agent_message.function_call\n",
    "                function_name = function_call.name\n",
    "                kwargs = json.loads(function_call.arguments)\n",
    "                \n",
    "                print(f\"[Agent calling function {function_name} with arguments {kwargs}]\\n\")\n",
    "                \n",
    "                if function_name == self.knowledge_base.function_name:\n",
    "                    function_output = self.knowledge_base.search(**kwargs)\n",
    "                    function_message = {\n",
    "                        \"role\": \"function\",\n",
    "                        \"name\": function_name,\n",
    "                        \"content\": function_output\n",
    "                    }\n",
    "                    self.memory.add(function_message)\n",
    "                \n",
    "                else:\n",
    "                    function_output = \"WARNING: Function not found!\"\n",
    "        \n",
    "        print(answer)\n",
    "\n",
    "    def generate_response(self):\n",
    "        \n",
    "        self.memory.delete_history()\n",
    "        \n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=self.model,\n",
    "            messages=self.memory.messages,\n",
    "            functions=[self.knowledge_base.function],\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        self.calculate_cost(response.usage)\n",
    "\n",
    "        agent_message = response.choices[0].message\n",
    "        \n",
    "        return agent_message\n",
    "    \n",
    "    def calculate_cost(self, usage):\n",
    "        cost = (usage.prompt_tokens * self.prompt_cost + usage.completion_tokens * self.completion_cost)/1000\n",
    "        self.cost_list.append(cost)\n",
    "        self.total_cost += cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203a0448-917d-4966-b8ec-1b7907d56aff",
   "metadata": {},
   "source": [
    "Finally, we need to make a modification to the `Memory` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a53dfd-5ee8-48dc-8ced-0e6636da15f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    \n",
    "    def __init__(self, initial_prompt=None, max_tokens=3000):\n",
    "        if initial_prompt is None:\n",
    "            initial_prompt = \"\"\n",
    "        self.messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": initial_prompt\n",
    "            }\n",
    "        ]\n",
    "        self.max_tokens = max_tokens\n",
    "        self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        \n",
    "    def add(self, message):\n",
    "        self.messages.append(dict(message))\n",
    "\n",
    "    def delete_history(self):\n",
    "        while True:\n",
    "            total_tokens = 0\n",
    "            for message in self.messages:\n",
    "                if message[\"content\"] != None:\n",
    "                    message_tokens = len(self.encoding.encode(message[\"content\"]))\n",
    "                    total_tokens += message_tokens\n",
    "            if total_tokens > self.max_tokens:\n",
    "                self.messages[1:] = self.messages[2:]\n",
    "            else:\n",
    "                return\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"\\n\".join([str(message) for message in self.messages])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e8a96a-b6f6-4ccb-a8a3-1d0887ee9ed1",
   "metadata": {},
   "source": [
    "Test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8da796-79d9-443f-9e29-e5ded15f118d",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_prompt = \"\"\"\n",
    "Answer the user's question/request: Consult the internal documentation by calling the function 'search_documentation'.\n",
    "\"\"\"\n",
    "\n",
    "memory = Memory(initial_prompt=initial_prompt)\n",
    "knowledge_base = KnowledgeBase(vdb)\n",
    "agent = BaseAgent(memory=memory, knowledge_base=knowledge_base, model='gpt-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a119c705-40af-4258-9c33-39185e24bbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.reply(\"Can I export luxury goods to Russia in 2023?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6790e255-8f5a-4647-9a89-409687d904e6",
   "metadata": {},
   "source": [
    "We can take a look at the memory, to see what happened:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e7a83c-3286-4f7d-bf4a-89b6e050b63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336c00f4-9ba3-450d-b918-d2e4e33f250f",
   "metadata": {},
   "source": [
    "# 6. A Webapp for your LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d46f56a-2f8c-42aa-9ca1-a0c9f3731de1",
   "metadata": {},
   "source": [
    "Now we have all the core pieces for our chatbot, and can therefore move out of the notebook to an actual script. What we want to do is\n",
    "\n",
    "1. Put some order in the code\n",
    "2. Give the chatbot an interface, i.e., build a webapp\n",
    "\n",
    "I have already done that: all the code can be found in the `src` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a36a824-c4fb-4b7c-93b4-aa6dd0acfb4f",
   "metadata": {},
   "source": [
    "# 7. Competition: Can your Chatbot Answer Correctly to our Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f399e123-b926-44d2-a598-e853dea58d87",
   "metadata": {},
   "source": [
    "Competition! You have 20 minutes to play around with the prompt (or, if you have other ideas, feel free to try them!): the purpose is to answer how many questions you can. Some questions will:\n",
    "\n",
    "- Be based on general knowledge.\n",
    "- Involve translations (when someone says 'English', do they mean British English or American English?)\n",
    "- Require a little mathematical reasoning.\n",
    "- Be related to the provided internal documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
